{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bebhawna/Handwritten-Character-Recognition/blob/main/DDModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uR5mmPhX3-cP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC_mPJSA4dfz",
        "outputId": "946e5129-9a85-4816-d33a-c2eb05f9cf87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yyOeGgm4q_P"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import cv2\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3jR1V2A4s8b"
      },
      "outputs": [],
      "source": [
        "base='/content/drive/MyDrive/Deepfake Detection/train_sample_videos'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdp1AyOG45M1",
        "outputId": "e1d51100-ebb0-45a1-eadf-9e573fd4ce4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "# # For better integrity and scalability and to ensure clealiness in code\n",
        "# def get_filename_only(file):\n",
        "#   file_basename=os.path.basename(file)\n",
        "#   filename_only=file_basename.split('.')[0]\n",
        "#   return filename_only\n",
        "# with open(os.path.join(base,'metadata.json')) as metadata_json:\n",
        "#   metadata=json.load(metadata_json)\n",
        "#   print(len(metadata))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxcLDKIQ4_YA",
        "outputId": "ad3b76a0-2780-4e01-c402-9740ccba2024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aagfhgtpmv.mp4\n",
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/train_sample_videos/aagfhgtpmv\n",
            "Converting Video to Images...\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Done!\n",
            "aapnvogymq.mp4\n",
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/train_sample_videos/aapnvogymq\n",
            "Converting Video to Images...\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Done!\n",
            "abarnvbtwb.mp4\n",
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/train_sample_videos/abarnvbtwb\n",
            "Converting Video to Images...\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Done!\n",
            "abofeumbvv.mp4\n",
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/train_sample_videos/abofeumbvv\n",
            "Converting Video to Images...\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Done!\n",
            "abqwwspghj.mp4\n",
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/train_sample_videos/abqwwspghj\n",
            "Converting Video to Images...\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Original Dimensions:  (1080, 1920, 3)\n",
            "Scale Ratio:  0.33\n",
            "Resized Dimensions:  (356, 633, 3)\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Creation of directory having mp4 extension video to process and capture the frame\n",
        "for filename in metadata.keys():\n",
        "    print(filename)\n",
        "    if (filename.endswith(\".mp4\")):\n",
        "        temporary_path = os.path.join(base, get_filename_only(filename))   #Attaching the file with the video and then creatign directory\n",
        "        print('Creating Directory: ' + temporary_path)\n",
        "        os.makedirs(temporary_path, exist_ok=True)\n",
        "        print('Converting Video to Images...')\n",
        "        count = 0\n",
        "        video_file = os.path.join(base, filename)\n",
        "        cap = cv2.VideoCapture(video_file)\n",
        "        frame_rate = cap.get(5)\n",
        "        #Initialising the loop to extract the frames from the video for better training\n",
        "        while(cap.isOpened()):\n",
        "            frame_id = cap.get(1) #current frame number\n",
        "            ret, frame = cap.read()\n",
        "            if (ret != True):\n",
        "                break\n",
        "                #Setting a particular limit in which the frame would be captured from the video\n",
        "            if (frame_id % math.floor(frame_rate) == 0):\n",
        "                print('Original Dimensions: ', frame.shape)\n",
        "                # Rescaling the frames for better resolution\n",
        "                if frame.shape[1] < 300:\n",
        "                    scale_ratio = 2\n",
        "                elif frame.shape[1] > 1900:\n",
        "                    scale_ratio = 0.33\n",
        "                elif frame.shape[1] > 1000 and frame.shape[1] <= 1900 :\n",
        "                    scale_ratio = 0.5\n",
        "                else:\n",
        "                    scale_ratio = 1\n",
        "                print('Scale Ratio: ', scale_ratio)\n",
        "                # Redefining the dimensions of the image\n",
        "                width = int(frame.shape[1] * scale_ratio)\n",
        "                height = int(frame.shape[0] * scale_ratio)\n",
        "                dim = (width, height)\n",
        "                new_frame = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n",
        "                print('Resized Dimensions: ', new_frame.shape)\n",
        "\n",
        "                new_filename = '{}-{:03d}.png'.format(os.path.join(temporary_path, get_filename_only(filename)), count)  #Storing or overlapping the new images on old path\n",
        "                count = count + 1\n",
        "                cv2.imwrite(new_filename, new_frame)\n",
        "        cap.release()  # Releasing the video when done\n",
        "        print(\"Done!\")\n",
        "    else:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCWG1VBN5e0n",
        "outputId": "c7465553-08a0-4180-cb26-4a627b2c4c71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mtcnn\n",
            "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (1.4.2)\n",
            "Collecting lz4>=4.3.3 (from mtcnn)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Downloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lz4, mtcnn\n",
            "Successfully installed lz4-4.3.3 mtcnn-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mtcnn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnA9aQAs5ivt"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from mtcnn import MTCNN\n",
        "import sys, os.path\n",
        "import json\n",
        "from keras import backend as K\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR798_s65v7w"
      },
      "outputs": [],
      "source": [
        "base='/content/drive/MyDrive/Deepfake Detection/train_sample_videos'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSBB2GB252n7",
        "outputId": "c7cab264-327e-484e-f7d0-1531e7d1b748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "Processing Directory: /content/drive/MyDrive/Deepfake Detection/train_sample_videos/aagfhgtpmv\n",
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/train_sample_videos/aagfhgtpmv/faces\n",
            "Cropping Faces from Images...\n",
            "Processing  aagfhgtpmv-000.png\n",
            "Face Detected:  1\n",
            "[110, 58, 71, 92]\n",
            "0.9999967813491821\n",
            "88 30 202 177\n",
            "Processing  aagfhgtpmv-001.png\n",
            "Face Detected:  1\n",
            "[167, 58, 72, 95]\n",
            "0.9999997615814209\n",
            "145 29 260 181\n",
            "Processing  aagfhgtpmv-002.png\n",
            "Face Detected:  1\n",
            "[202, 60, 66, 94]\n",
            "0.9998657703399658\n",
            "182 31 287 182\n",
            "Processing  aagfhgtpmv-003.png\n",
            "Face Detected:  1\n",
            "[319, 71, 65, 91]\n",
            "0.9997005462646484\n",
            "299 43 403 189\n",
            "Processing  aagfhgtpmv-004.png\n",
            "Face Detected:  1\n",
            "[368, 52, 71, 98]\n",
            "0.9994237422943115\n",
            "346 22 460 179\n",
            "Processing  aagfhgtpmv-005.png\n",
            "Face Detected:  1\n",
            "[340, 62, 61, 86]\n",
            "0.9994107484817505\n",
            "321 36 419 173\n",
            "Processing  aagfhgtpmv-006.png\n",
            "Face Detected:  1\n",
            "[350, 43, 63, 86]\n",
            "0.9989352822303772\n",
            "331 17 431 154\n",
            "Processing  aagfhgtpmv-007.png\n",
            "Face Detected:  1\n",
            "[302, 36, 67, 87]\n",
            "0.9999823570251465\n",
            "281 9 389 149\n",
            "Processing  aagfhgtpmv-008.png\n",
            "Face Detected:  1\n",
            "[326, 40, 66, 88]\n",
            "0.9994900226593018\n",
            "306 13 411 154\n",
            "Processing  aagfhgtpmv-009.png\n",
            "Face Detected:  1\n",
            "[257, 55, 64, 90]\n",
            "0.9998157024383545\n",
            "237 28 340 172\n",
            "Processing  aagfhgtpmv-010.png\n",
            "Face Detected:  1\n",
            "[160, 44, 71, 96]\n",
            "0.9988783001899719\n",
            "138 15 252 168\n",
            "Processing Directory: /content/drive/MyDrive/Deepfake Detection/train_sample_videos/aapnvogymq\n",
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/train_sample_videos/aapnvogymq/faces\n",
            "Cropping Faces from Images...\n",
            "Processing  aapnvogymq-000.png\n",
            "Face Detected:  2\n",
            "[392, 60, 48, 63]\n",
            "0.999997079372406\n",
            "377 41 454 141\n",
            "[190, 78, 44, 69]\n",
            "0.9997735023498535\n",
            "176 57 247 167\n",
            "Processing  aapnvogymq-001.png\n",
            "Face Detected:  2\n",
            "[395, 63, 46, 61]\n",
            "0.9999997615814209\n",
            "381 44 454 142\n",
            "[183, 77, 44, 65]\n",
            "0.9999126195907593\n",
            "169 57 240 161\n",
            "Processing  aapnvogymq-002.png\n",
            "Face Detected:  2\n",
            "[390, 63, 46, 59]\n",
            "0.9999997615814209\n",
            "376 45 449 139\n",
            "[184, 74, 46, 68]\n",
            "0.9988875985145569\n",
            "170 53 243 162\n",
            "Processing  aapnvogymq-003.png\n",
            "Face Detected:  2\n",
            "[388, 62, 46, 60]\n",
            "0.9999997615814209\n",
            "374 44 447 140\n",
            "[182, 74, 46, 66]\n",
            "0.9994813799858093\n",
            "168 54 241 159\n",
            "Processing  aapnvogymq-004.png\n",
            "Face Detected:  2\n",
            "[387, 62, 46, 61]\n",
            "0.9999979138374329\n",
            "373 43 446 141\n",
            "[180, 75, 48, 68]\n",
            "0.9997851848602295\n",
            "165 54 242 163\n",
            "Processing  aapnvogymq-005.png\n",
            "Face Detected:  2\n",
            "[385, 62, 47, 62]\n",
            "0.999994158744812\n",
            "370 43 446 142\n",
            "[177, 79, 47, 67]\n",
            "0.9940614104270935\n",
            "162 58 238 166\n",
            "Processing  aapnvogymq-006.png\n",
            "Face Detected:  2\n",
            "[383, 63, 47, 61]\n",
            "0.9999963045120239\n",
            "368 44 444 142\n",
            "[186, 77, 42, 68]\n",
            "0.9999268054962158\n",
            "173 56 240 165\n",
            "Processing  aapnvogymq-007.png\n",
            "Face Detected:  2\n",
            "[383, 63, 52, 62]\n",
            "0.9999879598617554\n",
            "367 44 450 143\n",
            "[174, 76, 45, 64]\n",
            "0.9998979568481445\n",
            "160 56 232 159\n",
            "Processing  aapnvogymq-008.png\n",
            "Face Detected:  2\n",
            "[382, 61, 46, 60]\n",
            "0.999997615814209\n",
            "368 43 441 139\n",
            "[177, 74, 47, 63]\n",
            "0.9991669654846191\n",
            "162 55 238 155\n",
            "Processing  aapnvogymq-009.png\n",
            "Face Detected:  3\n",
            "[388, 61, 46, 61]\n",
            "0.9999988079071045\n",
            "374 42 447 140\n",
            "[179, 80, 45, 64]\n",
            "0.99992835521698\n",
            "165 60 237 163\n",
            "[219, 312, 34, 43]\n",
            "0.8602622151374817\n",
            "Skipped a face..\n",
            "Processing  aapnvogymq-010.png\n",
            "Face Detected:  2\n",
            "[391, 62, 43, 59]\n",
            "0.9999535083770752\n",
            "378 44 446 138\n",
            "[179, 77, 45, 65]\n",
            "0.9997797608375549\n",
            "165 57 237 161\n",
            "Processing Directory: /content/drive/MyDrive/Deepfake Detection/train_sample_videos/abarnvbtwb\n",
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/train_sample_videos/abarnvbtwb/faces\n",
            "Cropping Faces from Images...\n",
            "Processing  abarnvbtwb-000.png\n",
            "Face Detected:  1\n",
            "[262, 65, 62, 64]\n",
            "0.9938739538192749\n",
            "243 45 342 148\n",
            "Processing  abarnvbtwb-001.png\n",
            "Face Detected:  1\n",
            "[293, 52, 59, 78]\n",
            "0.9999996423721313\n",
            "275 28 369 153\n",
            "Processing  abarnvbtwb-002.png\n",
            "Face Detected:  1\n",
            "[302, 51, 60, 77]\n",
            "1.0\n",
            "284 27 380 151\n",
            "Processing  abarnvbtwb-003.png\n",
            "Face Detected:  1\n",
            "[298, 51, 59, 77]\n",
            "0.999990701675415\n",
            "280 27 374 151\n",
            "Processing  abarnvbtwb-004.png\n",
            "Face Detected:  1\n",
            "[292, 50, 60, 76]\n",
            "0.9999929666519165\n",
            "274 27 370 148\n",
            "Processing  abarnvbtwb-005.png\n",
            "Face Detected:  1\n",
            "[296, 64, 60, 80]\n",
            "0.9999884366989136\n",
            "278 40 374 168\n",
            "Processing  abarnvbtwb-006.png\n",
            "Face Detected:  1\n",
            "[294, 59, 62, 85]\n",
            "0.9997045397758484\n",
            "275 33 374 169\n",
            "Processing  abarnvbtwb-007.png\n",
            "Face Detected:  1\n",
            "[294, 60, 58, 78]\n",
            "0.9999996423721313\n",
            "276 36 369 161\n",
            "Processing  abarnvbtwb-008.png\n",
            "Face Detected:  1\n",
            "[294, 58, 62, 81]\n",
            "0.9999960660934448\n",
            "275 33 374 163\n",
            "Processing  abarnvbtwb-009.png\n",
            "Face Detected:  1\n",
            "[303, 54, 61, 81]\n",
            "0.9999845027923584\n",
            "284 29 382 159\n",
            "Processing  abarnvbtwb-010.png\n",
            "Face Detected:  1\n",
            "[307, 55, 64, 83]\n",
            "0.9999854564666748\n",
            "287 30 390 162\n",
            "Processing Directory: /content/drive/MyDrive/Deepfake Detection/train_sample_videos/abofeumbvv\n",
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/train_sample_videos/abofeumbvv/faces\n",
            "Cropping Faces from Images...\n",
            "Processing  abofeumbvv-000.png\n",
            "Face Detected:  0\n",
            "Processing  abofeumbvv-001.png\n",
            "Face Detected:  0\n",
            "Processing  abofeumbvv-002.png\n",
            "Face Detected:  0\n",
            "Processing  abofeumbvv-003.png\n",
            "Face Detected:  1\n",
            "[293, 51, 36, 45]\n",
            "0.8861744403839111\n",
            "282 37 339 109\n",
            "Processing  abofeumbvv-004.png\n",
            "Face Detected:  1\n",
            "[294, 52, 36, 46]\n",
            "0.905790388584137\n",
            "283 38 340 111\n",
            "Processing  abofeumbvv-005.png\n",
            "Face Detected:  1\n",
            "[294, 53, 35, 45]\n",
            "0.8120918273925781\n",
            "283 39 339 111\n",
            "Processing  abofeumbvv-006.png\n",
            "Face Detected:  1\n",
            "[296, 53, 35, 45]\n",
            "0.834635317325592\n",
            "285 39 341 111\n",
            "Processing  abofeumbvv-007.png\n",
            "Face Detected:  0\n",
            "Processing  abofeumbvv-008.png\n",
            "Face Detected:  1\n",
            "[298, 50, 36, 45]\n",
            "0.8889635801315308\n",
            "287 36 344 108\n",
            "Processing  abofeumbvv-009.png\n",
            "Face Detected:  1\n",
            "[298, 54, 35, 44]\n",
            "0.9287840723991394\n",
            "287 40 343 111\n",
            "Processing  abofeumbvv-010.png\n",
            "Face Detected:  0\n",
            "Processing Directory: /content/drive/MyDrive/Deepfake Detection/train_sample_videos/abqwwspghj\n",
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/train_sample_videos/abqwwspghj/faces\n",
            "Cropping Faces from Images...\n",
            "Processing  abqwwspghj-000.png\n",
            "Face Detected:  1\n",
            "[264, 30, 55, 73]\n",
            "0.9996711015701294\n",
            "247 8 335 124\n",
            "Processing  abqwwspghj-001.png\n",
            "Face Detected:  1\n",
            "[260, 27, 57, 77]\n",
            "0.9999769926071167\n",
            "242 3 334 127\n",
            "Processing  abqwwspghj-002.png\n",
            "Face Detected:  1\n",
            "[280, 29, 53, 74]\n",
            "0.9990971088409424\n",
            "264 6 348 125\n",
            "Processing  abqwwspghj-003.png\n",
            "Face Detected:  1\n",
            "[265, 28, 54, 69]\n",
            "0.9991342425346375\n",
            "248 7 335 117\n",
            "Processing  abqwwspghj-004.png\n",
            "Face Detected:  1\n",
            "[267, 29, 54, 67]\n",
            "0.9996955394744873\n",
            "250 8 337 116\n",
            "Processing  abqwwspghj-005.png\n",
            "Face Detected:  1\n",
            "[267, 28, 55, 73]\n",
            "0.9991594552993774\n",
            "250 6 338 122\n",
            "Processing  abqwwspghj-006.png\n",
            "Face Detected:  1\n",
            "[270, 30, 54, 70]\n",
            "0.9986799359321594\n",
            "253 9 340 121\n",
            "Processing  abqwwspghj-007.png\n",
            "Face Detected:  1\n",
            "[281, 26, 54, 70]\n",
            "0.9990137815475464\n",
            "264 5 351 117\n",
            "Processing  abqwwspghj-008.png\n",
            "Face Detected:  1\n",
            "[285, 27, 54, 68]\n",
            "0.9995344877243042\n",
            "268 6 355 115\n",
            "Processing  abqwwspghj-009.png\n",
            "Face Detected:  1\n",
            "[283, 30, 54, 75]\n",
            "0.9979670643806458\n",
            "266 7 353 127\n",
            "Processing  abqwwspghj-010.png\n",
            "Face Detected:  1\n",
            "[269, 28, 54, 73]\n",
            "0.9958515167236328\n",
            "252 6 339 122\n"
          ]
        }
      ],
      "source": [
        "#To get the name of the file instead of complete path\n",
        "def get_filename_only(file_path):\n",
        "    file_basename = os.path.basename(file_path)\n",
        "    filename_only = file_basename.split('.')[0]\n",
        "    return filename_only\n",
        "\n",
        "with open(os.path.join(base, 'metadata.json')) as metadata_json:\n",
        "    metadata = json.load(metadata_json)\n",
        "    print(len(metadata))\n",
        "\n",
        "for filename in metadata.keys():\n",
        "    temp_path = os.path.join(base, get_filename_only(filename))\n",
        "    print('Processing Directory: ' + temp_path)\n",
        "    if not os.path.exists(temp_path):\n",
        "           print(f\"Directory does not exist: {temp_path}. Extracting frames from video...\")\n",
        "           continue\n",
        "    frame_images = [x for x in os.listdir(temp_path) if os.path.isfile(os.path.join(temp_path, x))]\n",
        "    faces_path = os.path.join(temp_path, 'faces')\n",
        "    print('Creating Directory: ' + faces_path)\n",
        "    os.makedirs(faces_path, exist_ok=True)\n",
        "    print('Cropping Faces from Images...')\n",
        "\n",
        "    for frame in frame_images:\n",
        "        print('Processing ', frame)\n",
        "        detector = MTCNN()\n",
        "        image = cv2.cvtColor(cv2.imread(os.path.join(temp_path, frame)), cv2.COLOR_BGR2RGB)\n",
        "        results = detector.detect_faces(image)\n",
        "        print('Face Detected: ', len(results))\n",
        "        count = 0\n",
        "\n",
        "        for result in results:\n",
        "            bounding_box = result['box']\n",
        "            print(bounding_box)\n",
        "            confidence = result['confidence']\n",
        "            print(confidence)\n",
        "            if len(results) < 2 or confidence > 0.95:\n",
        "                margin_x = bounding_box[2] * 0.3  # 30% as the margin\n",
        "                margin_y = bounding_box[3] * 0.3  # 30% as the margin\n",
        "                x1 = int(bounding_box[0] - margin_x)\n",
        "                if x1 < 0:\n",
        "                    x1 = 0\n",
        "                x2 = int(bounding_box[0] + bounding_box[2] + margin_x)\n",
        "                if x2 > image.shape[1]:\n",
        "                    x2 = image.shape[1]\n",
        "                y1 = int(bounding_box[1] - margin_y)\n",
        "                if y1 < 0:\n",
        "                    y1 = 0\n",
        "                y2 = int(bounding_box[1] + bounding_box[3] + margin_y)\n",
        "                if y2 > image.shape[0]:\n",
        "                    y2 = image.shape[0]\n",
        "                print(x1, y1, x2, y2)\n",
        "                crop_image = image[y1:y2, x1:x2]\n",
        "                new_filename = '{}-{:02d}.png'.format(os.path.join(faces_path, get_filename_only(frame)), count)\n",
        "                count = count + 1\n",
        "                cv2.imwrite(new_filename, cv2.cvtColor(crop_image, cv2.COLOR_RGB2BGR))\n",
        "            else:\n",
        "                print('Skipped a face..')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjsPOtsA6aSS",
        "outputId": "7cd65042-d416-4998-8e31-e1d9f0b108a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.10/dist-packages (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install split-folders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2OPlmRh6cay"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from distutils.dir_util import copy_tree\n",
        "import shutil\n",
        "import numpy as np\n",
        "import splitfolders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIfKtbGR6fw2",
        "outputId": "4acc2d73-b7b7-40e5-ddf8-618028685cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/prepared_dataset\n"
          ]
        }
      ],
      "source": [
        "\n",
        "base_path = '/content/drive/MyDrive/Deepfake Detection/train_sample_videos'\n",
        "dataset_path = '/content/drive/MyDrive/Deepfake Detection/prepared_dataset'\n",
        "print('Creating Directory: ' + dataset_path)\n",
        "os.makedirs(dataset_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-tzqMb96h7U",
        "outputId": "11145d8b-b479-47e7-e723-80fbe2db4706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/tmp_fake_faces\n",
            "5\n",
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/prepared_dataset/real\n",
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/prepared_dataset/fake\n",
            "aagfhgtpmv.mp4\n",
            "FAKE\n",
            "/content/drive/MyDrive/Deepfake Detection/train_sample_videos/aagfhgtpmv/faces\n",
            "Copying to :/content/drive/MyDrive/Deepfake Detection/tmp_fake_faces\n",
            "aapnvogymq.mp4\n",
            "FAKE\n",
            "/content/drive/MyDrive/Deepfake Detection/train_sample_videos/aapnvogymq/faces\n",
            "Copying to :/content/drive/MyDrive/Deepfake Detection/tmp_fake_faces\n",
            "abarnvbtwb.mp4\n",
            "REAL\n",
            "/content/drive/MyDrive/Deepfake Detection/train_sample_videos/abarnvbtwb/faces\n",
            "Copying to :/content/drive/MyDrive/Deepfake Detection/prepared_dataset/real\n",
            "abofeumbvv.mp4\n",
            "FAKE\n",
            "/content/drive/MyDrive/Deepfake Detection/train_sample_videos/abofeumbvv/faces\n",
            "Copying to :/content/drive/MyDrive/Deepfake Detection/tmp_fake_faces\n",
            "abqwwspghj.mp4\n",
            "FAKE\n",
            "/content/drive/MyDrive/Deepfake Detection/train_sample_videos/abqwwspghj/faces\n",
            "Copying to :/content/drive/MyDrive/Deepfake Detection/tmp_fake_faces\n",
            "Total Number of Real faces:  11\n",
            "Total Number of Fake faces:  50\n",
            "Down-sampling Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 35 files [00:00, 240.43 files/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/ Val/ Test Split Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tmp_fake_path = '/content/drive/MyDrive/Deepfake Detection/tmp_fake_faces'\n",
        "print('Creating Directory: ' + tmp_fake_path)\n",
        "os.makedirs(tmp_fake_path, exist_ok=True)\n",
        "\n",
        "def get_filename_only(file_path):\n",
        "    file_basename = os.path.basename(file_path)\n",
        "    filename_only = file_basename.split('.')[0]\n",
        "    return filename_only\n",
        "\n",
        "with open(os.path.join(base_path, 'metadata.json')) as metadata_json:\n",
        "    metadata = json.load(metadata_json)\n",
        "    print(len(metadata))\n",
        "\n",
        "real_path = os.path.join(dataset_path, 'real')\n",
        "print('Creating Directory: ' + real_path)\n",
        "os.makedirs(real_path, exist_ok=True)\n",
        "\n",
        "fake_path = os.path.join(dataset_path, 'fake')\n",
        "print('Creating Directory: ' + fake_path)\n",
        "os.makedirs(fake_path, exist_ok=True)\n",
        "\n",
        "for filename in metadata.keys():\n",
        "    print(filename)\n",
        "    print(metadata[filename]['label'])\n",
        "    tmp_path = os.path.join(os.path.join(base_path, get_filename_only(filename)), 'faces')\n",
        "    print(tmp_path)\n",
        "    if os.path.exists(tmp_path):\n",
        "        if metadata[filename]['label'] == 'REAL':\n",
        "            print('Copying to :' + real_path)\n",
        "            copy_tree(tmp_path, real_path)\n",
        "        elif metadata[filename]['label'] == 'FAKE':\n",
        "            print('Copying to :' + tmp_fake_path)\n",
        "            copy_tree(tmp_path, tmp_fake_path)\n",
        "        else:\n",
        "            print('Ignored..')\n",
        "\n",
        "all_real_faces = [f for f in os.listdir(real_path) if os.path.isfile(os.path.join(real_path, f))]\n",
        "print('Total Number of Real faces: ', len(all_real_faces))\n",
        "\n",
        "all_fake_faces = [f for f in os.listdir(tmp_fake_path) if os.path.isfile(os.path.join(tmp_fake_path, f))]\n",
        "print('Total Number of Fake faces: ', len(all_fake_faces))\n",
        "\n",
        "random_faces = np.random.choice(all_fake_faces, len(all_real_faces), replace=False)\n",
        "for fname in random_faces:\n",
        "    src = os.path.join(tmp_fake_path, fname)\n",
        "    dst = os.path.join(fake_path, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n",
        "print('Down-sampling Done!')\n",
        "\n",
        "# Split into Train/ Val/ Test folders\n",
        "splitfolders.ratio(dataset_path, output='split_dataset', seed=1377, ratio=(.8, .1, .1)) # default values\n",
        "print('Train/ Val/ Test Split Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXnqlNpW7fik"
      },
      "outputs": [],
      "source": [
        "\n",
        "from distutils.dir_util import copy_tree\n",
        "import shutil\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga3_vBkO7jRS",
        "outputId": "d8ba4305-1079-466a-a125-9bd434562912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version:  2.17.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "print('TensorFlow version: ', tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q37DPhYD7laB"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/drive/MyDrive/Deepfake Detection/split_dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zb2LWcP8-dw",
        "outputId": "f07d1539-db18-4cf7-be47-31b1d8f9ddc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/tmp_debug\n"
          ]
        }
      ],
      "source": [
        "tmp_debug_path = '/content/drive/MyDrive/Deepfake Detection/tmp_debug'\n",
        "print('Creating Directory: ' + tmp_debug_path)\n",
        "os.makedirs(tmp_debug_path, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBqLeUJH9GhR"
      },
      "outputs": [],
      "source": [
        "def get_filename_only(file_path):\n",
        "    file_basename = os.path.basename(file_path)\n",
        "    filename_only = file_basename.split('.')[0]\n",
        "    return filename_only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvfH2G4A9KfL",
        "outputId": "c05436a4-00df-4151-a2a7-30d9764d5528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: efficientnet in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from efficientnet) (1.0.8)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from efficientnet) (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.26.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.12.1)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (1.13.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (11.1.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (2024.12.12)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->efficientnet) (0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install efficientnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYoVSCYX9NNO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import applications\n",
        "from efficientnet.tfkeras import EfficientNetB0\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUKtasCe9RKL"
      },
      "outputs": [],
      "source": [
        "input_size=128\n",
        "batch_size_num=32\n",
        "train_path=os.path.join(dataset_path,'train')\n",
        "val_path=os.path.join(dataset_path,'val')\n",
        "test_path=os.path.join(dataset_path,'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZk9KIMm9Tvv",
        "outputId": "55808ae2-87a6-43d5-d165-c0616aa09cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16 images belonging to 2 classes.\n",
            "Found 2 images belonging to 2 classes.\n",
            "Found 4 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen=ImageDataGenerator(\n",
        "    rescale= 1/255,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        ")\n",
        "train_generator= train_datagen.flow_from_directory(\n",
        "    directory=train_path,\n",
        "    target_size=(input_size,input_size),\n",
        "    color_mode='rgb',\n",
        "    class_mode=\"binary\",\n",
        "    batch_size=batch_size_num,\n",
        "    shuffle=True\n",
        ")\n",
        "val_datagen=ImageDataGenerator(\n",
        "    rescale=1/255\n",
        ")\n",
        "val_generator=val_datagen.flow_from_directory(\n",
        "    directory=val_path,\n",
        "    target_size=(input_size,input_size),\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode=\"binary\",\n",
        "    batch_size=batch_size_num,\n",
        "    shuffle=True,\n",
        ")\n",
        "test_datagen=ImageDataGenerator(\n",
        "    rescale=1/255\n",
        ")\n",
        "test_generator=test_datagen.flow_from_directory(\n",
        "    directory=test_path,\n",
        "    classes=['real','fake'],\n",
        "    target_size=(input_size, input_size),\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode=None,\n",
        "    batch_size=1,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "DFZ0bhyfGIeQ",
        "outputId": "307740bd-c30e-4523-84fe-6ff3aaf5522b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ efficientnet-b0 (\u001b[38;5;33mFunctional\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │       \u001b[38;5;34m4,049,564\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m655,872\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m65,664\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ efficientnet-b0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,564</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,771,229\u001b[0m (18.20 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,771,229</span> (18.20 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,729,213\u001b[0m (18.04 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,729,213</span> (18.04 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m42,016\u001b[0m (164.12 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,016</span> (164.12 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "efficient_net = EfficientNetB0(\n",
        "    weights = 'imagenet',\n",
        "    input_shape = (input_size, input_size, 3),\n",
        "    include_top = False,\n",
        "    pooling = 'max'\n",
        ")\n",
        "\n",
        "model = Sequential()\n",
        "model.add(efficient_net)\n",
        "model.add(Dense(units = 512, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units = 128, activation = 'relu'))\n",
        "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
        "model.summary()\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.get_logger().setLevel('ERROR')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDXlLveFGTuq",
        "outputId": "651c4aaa-13dd-44b6-e637-5be59c51c2b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Directory: /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer = Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/Deepfake Detection/tmp_checkpoint'\n",
        "print('Creating Directory: ' + checkpoint_filepath)\n",
        "os.makedirs(checkpoint_filepath, exist_ok=True)\n",
        "\n",
        "custom_callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor = 'val_loss',\n",
        "        mode = 'min',\n",
        "        patience = 5,\n",
        "        verbose = 1\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        filepath = os.path.join(checkpoint_filepath, 'best_model.keras'),\n",
        "        monitor = 'val_loss',\n",
        "        mode = 'min',\n",
        "        verbose = 1,\n",
        "        save_best_only = True\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGAsuZ99HBpP",
        "outputId": "6ad39406-35f6-4d16-9a1e-f2f197074a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40s/step - accuracy: 1.0000 - loss: 0.0013\n",
            "Epoch 1: val_loss improved from 0.01107 to 0.00971, saving model to /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint/best_model.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 44s/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 0.0097\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 3/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
            "  current = self.get_monitor_value(logs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_loss available, skipping.\n",
            "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0015\n",
            "Epoch 3: val_loss improved from 0.00971 to 0.00849, saving model to /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint/best_model.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 1.0000 - val_loss: 0.0085\n",
            "Epoch 4/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 5/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 8.3825e-04\n",
            "Epoch 5: val_loss improved from 0.00849 to 0.00735, saving model to /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint/best_model.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - accuracy: 1.0000 - loss: 8.3825e-04 - val_accuracy: 1.0000 - val_loss: 0.0074\n",
            "Epoch 6/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 7/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 3.8852e-04\n",
            "Epoch 7: val_loss improved from 0.00735 to 0.00651, saving model to /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint/best_model.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 3.8852e-04 - val_accuracy: 1.0000 - val_loss: 0.0065\n",
            "Epoch 8/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 9/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0035\n",
            "Epoch 9: val_loss improved from 0.00651 to 0.00581, saving model to /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint/best_model.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0058\n",
            "Epoch 10/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 11/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 6.4787e-04\n",
            "Epoch 11: val_loss improved from 0.00581 to 0.00518, saving model to /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint/best_model.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 6.4787e-04 - val_accuracy: 1.0000 - val_loss: 0.0052\n",
            "Epoch 12/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 13/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.0322e-04\n",
            "Epoch 13: val_loss improved from 0.00518 to 0.00467, saving model to /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint/best_model.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 1.0322e-04 - val_accuracy: 1.0000 - val_loss: 0.0047\n",
            "Epoch 14/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 15/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0013\n",
            "Epoch 15: val_loss improved from 0.00467 to 0.00421, saving model to /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint/best_model.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 0.0042\n",
            "Epoch 16/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 17/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 8.5677e-05\n",
            "Epoch 17: val_loss improved from 0.00421 to 0.00390, saving model to /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint/best_model.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 8.5677e-05 - val_accuracy: 1.0000 - val_loss: 0.0039\n",
            "Epoch 18/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 19/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 6.6074e-04\n",
            "Epoch 19: val_loss improved from 0.00390 to 0.00363, saving model to /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint/best_model.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 6.6074e-04 - val_accuracy: 1.0000 - val_loss: 0.0036\n",
            "Epoch 20/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 21/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 1.6778e-04\n",
            "Epoch 21: val_loss improved from 0.00363 to 0.00337, saving model to /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint/best_model.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 1.0000 - loss: 1.6778e-04 - val_accuracy: 1.0000 - val_loss: 0.0034\n",
            "Epoch 22/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 23/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 5.6143e-04\n",
            "Epoch 23: val_loss improved from 0.00337 to 0.00321, saving model to /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint/best_model.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 5.6143e-04 - val_accuracy: 1.0000 - val_loss: 0.0032\n",
            "Epoch 24/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 25/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.4434e-04\n",
            "Epoch 25: val_loss improved from 0.00321 to 0.00302, saving model to /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint/best_model.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.4434e-04 - val_accuracy: 1.0000 - val_loss: 0.0030\n",
            "Epoch 26/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 27/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0010\n",
            "Epoch 27: val_loss improved from 0.00302 to 0.00288, saving model to /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint/best_model.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 1.0000 - val_loss: 0.0029\n",
            "Epoch 28/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "Epoch 29/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0137\n",
            "Epoch 29: val_loss improved from 0.00288 to 0.00268, saving model to /content/drive/MyDrive/Deepfake Detection/tmp_checkpoint/best_model.keras\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0137 - val_accuracy: 1.0000 - val_loss: 0.0027\n",
            "Epoch 30/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
            "{'accuracy': [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0], 'loss': [0.001339917303994298, 0.0, 0.0015427495818585157, 0.0, 0.0008382463711313903, 0.0, 0.00038851576391607523, 0.0, 0.0034586144611239433, 0.0, 0.0006478738505393267, 0.0, 0.00010322338494006544, 0.0, 0.0013422387419268489, 0.0, 8.567656186642125e-05, 0.0, 0.0006607421091757715, 0.0, 0.0001677787658991292, 0.0, 0.0005614280817098916, 0.0, 0.00014433625619858503, 0.0, 0.001034644665196538, 0.0, 0.013652537018060684, 0.0], 'val_accuracy': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.0097053786739707, 0.008492385037243366, 0.007353936322033405, 0.006513547617942095, 0.00581016018986702, 0.0051826066337525845, 0.004667047411203384, 0.004206997342407703, 0.003904582466930151, 0.003626542864367366, 0.0033709686249494553, 0.003205053275451064, 0.0030216004233807325, 0.002882731147110462, 0.00267852027900517]}\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs = num_epochs,\n",
        "    steps_per_epoch = len(train_generator),\n",
        "    validation_data = val_generator,\n",
        "    validation_steps = len(val_generator),\n",
        "    callbacks = custom_callbacks\n",
        ")\n",
        "print(history.history)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = load_model(os.path.join(checkpoint_filepath, 'best_model.keras'))"
      ],
      "metadata": {
        "id": "92fBBCWr1oMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHdlE8lmHG0o"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "train_epochs=range(len(acc))\n",
        "val_epochs=range(len(val_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za6dAZnvHO6G",
        "outputId": "dd8a598c-ed57-47da-96b5-6bf26776bb85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[1 1]\n",
            " [0 4]]\n",
            "Accuracy: 83.33%\n",
            "Precision: 80.00%\n",
            "Recall: 100.00%\n",
            "F1 Score: 88.89%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# True labels and predicted labels\n",
        "for images,labels in validation_ds:\n",
        "  predicted_labels_batches=model.predict(images)\n",
        "  predicted_labels_batches=tf.argmax(predicted_labels_batches,axis=1)\n",
        "  test_labels.append(labels.numpy())\n",
        "  predicted_labels.append(predicted_labels_batches.numpy())\n",
        "test_labels=np.concatenate(test_labels)\n",
        "predicted_labels=np.concatenate(predicted_labels)\n",
        "\n",
        "if len(test_labels.shape)>1:\n",
        "  test_labels=np.argmax(test_labels,axis=1)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "# Precision\n",
        "precision = precision_score(true_labels, predicted_labels, pos_label='fake')\n",
        "\n",
        "# Recall\n",
        "recall = recall_score(true_labels, predicted_labels, pos_label='fake')\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(true_labels, predicted_labels, pos_label='fake')\n",
        "\n",
        "# Confusion Matrix (optional, for verification)\n",
        "cm = confusion_matrix(true_labels, predicted_labels, labels=['real', 'fake'])\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYNCr_78HTyf"
      },
      "outputs": [],
      "source": [
        "test_generator.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4gbcL5hHUW2",
        "outputId": "d304ab59-1910-476c-864c-3cdab12eca04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step\n",
            "                     Filename  Prediction\n",
            "0  real/abarnvbtwb-000-00.png    0.660621\n",
            "1  real/abarnvbtwb-002-00.png    0.999703\n",
            "2  fake/aagfhgtpmv-001-00.png    0.018093\n",
            "3  fake/aagfhgtpmv-009-00.png    0.152356\n"
          ]
        }
      ],
      "source": [
        "preds=best_model.predict(\n",
        "    test_generator,\n",
        "    verbose=1\n",
        ")\n",
        "test_result=pd.DataFrame({\n",
        "    \"Filename\": test_generator.filenames,\n",
        "    \"Prediction\": preds.flatten()\n",
        "})\n",
        "print(test_result)"
      ]
    },
    {
      "source": [
        "print(f\"Filename: {filename}, Prediction: {pred[0]:.6f}, Classified as: {label}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvVKwD4KZ_v_",
        "outputId": "e7a0b5b5-c871-4f90-c009-2065b9cb4e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filename: real/abarnvbtwb-000-00.png, Prediction: 0.660621, Classified as: real\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Upload a file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load the best model for prediction\n",
        "checkpoint_filepath = '/content/drive/MyDrive/Deepfake Detection/tmp_checkpoint'\n",
        "best_model_path = os.path.join(checkpoint_filepath, 'best_model.keras')\n",
        "model = load_model(best_model_path)\n",
        "\n",
        "# Function to preprocess and predict a single image\n",
        "def preprocess_and_predict(image_path):\n",
        "    image = load_img(image_path, target_size=(128, 128))\n",
        "    image = img_to_array(image)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = image / 255.0  # Normalize the image\n",
        "\n",
        "    prediction = model.predict(image)\n",
        "    if prediction[0] < 0.5:\n",
        "        return 'Fake', prediction[0]\n",
        "    else:\n",
        "        return 'Real', prediction[0]\n",
        "\n",
        "# Function to extract frames from a video\n",
        "def extract_frames_from_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = 0\n",
        "    frames = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame_filename = f\"/tmp/frame_{frame_count:04d}.jpg\"\n",
        "        cv2.imwrite(frame_filename, frame)\n",
        "        frames.append(frame_filename)\n",
        "        frame_count += 1\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "# Handle uploaded files (Image or Video)\n",
        "uploaded_file_path = list(uploaded.keys())[0]\n",
        "file_extension = os.path.splitext(uploaded_file_path)[1].lower()\n",
        "\n",
        "if file_extension in ['.jpg', '.jpeg', '.png']:\n",
        "    print(\"File is an image. Processing image...\")\n",
        "    label, confidence = preprocess_and_predict(uploaded_file_path)\n",
        "    print(f\"Prediction: {label} with confidence {confidence[0]:.2f}\") # Access the element using [0]\n",
        "elif file_extension in ['.mp4', '.avi', '.mov']:\n",
        "    print(\"File is a video. Extracting frames...\")\n",
        "    frames = extract_frames_from_video(uploaded_file_path)\n",
        "    # Predict on the first frame\n",
        "    label, confidence = preprocess_and_predict(frames[0])\n",
        "    print(f\"Prediction for the first frame: {label} with confidence {confidence[0]:.2f}\") # Access the element using [0]\n",
        "else:\n",
        "    print(\"Unsupported file format. Please upload an image or video.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "ZYS5EgICaKOG",
        "outputId": "58f4f09f-af89-4469-89c4-7157b76b676d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fe940bec-be27-4252-86e4-1ac4a35c7809\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fe940bec-be27-4252-86e4-1ac4a35c7809\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test1.png to test1 (6).png\n",
            "File is an image. Processing image...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "Prediction: Fake with confidence 0.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdDhWkvT7Gkd"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1oT1l0j2UkHHVTr5zcjvurSehnQfBK8_J",
      "authorship_tag": "ABX9TyOteCr+mVgJvKOqzFKTRGO0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}